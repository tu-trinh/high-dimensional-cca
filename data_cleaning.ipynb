{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97202574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = \"daily\"\n",
    "files = [f for f in listdir(f\"raw_data/{frequency}/\") if isfile(join(f\"raw_data/{frequency}/\", f)) and f.lower().endswith(\".csv\")]\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_header_row(full_path):\n",
    "    try:\n",
    "        header_pattern = re.compile(r\"^,[ \\w-]+.*$\")\n",
    "        with open(full_path, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if header_pattern.match(line.strip()):\n",
    "                    return i\n",
    "    except Exception as e:\n",
    "        print(\"Failed to find header row in\", full_path)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, daily = False):\n",
    "    date_col = \"Unnamed: 0\" if \"Unnamed: 0\" in df.columns else \" \"\n",
    "    df[date_col] = df[date_col].astype(str).str.strip()\n",
    "    if daily:\n",
    "        df = df[df[date_col].str.len() > 6]\n",
    "    else:\n",
    "        df = df[df[date_col].str.len() > 4]\n",
    "    df[date_col] = df[date_col].astype(str)\n",
    "    df[\"Year\"] = df[date_col].str[:4]\n",
    "    df[\"Month\"] = df[date_col].str[4:6]\n",
    "    if daily:\n",
    "        df[\"Day\"] = df[date_col].str[6:]\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors = \"coerce\")\n",
    "    df = df.drop(columns = [date_col])\n",
    "    df = df.dropna()\n",
    "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "    df[\"Month\"] = df[\"Month\"].astype(int)\n",
    "    if daily:\n",
    "        df[\"Day\"] = df[\"Day\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba89347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(data_path):\n",
    "    full_path = f\"raw_data/{frequency}/\" + data_path\n",
    "    header_row = find_header_row(full_path)\n",
    "    try:\n",
    "        data = pd.read_csv(full_path, skiprows = header_row, na_values = [\"-99.99\", \"-999\", -99.99, -999])\n",
    "        df = clean_data(data, frequency == \"daily\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Failed to read CSV for\", data_path, \"expected header row was\", header_row)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_name_prefix(name):\n",
    "    name = name.lower()\n",
    "    if \"industry_portfolios\" in name:\n",
    "        return \"us_\" + re.match(r\"^(\\d+)_\", name).group(1)\n",
    "    elif \"25_portfolios\" in name:\n",
    "        if name == \"25_portfolios\":\n",
    "            return \"na_100\"\n",
    "        elif \"developed_25_portfolios\" in name:\n",
    "            return \"dev_100\"\n",
    "        elif \"ex_us\" in name:\n",
    "            return \"dev_exUS_100\"\n",
    "        elif \"europe\" in name:\n",
    "            return \"euro_100\"\n",
    "        elif \"japan\" in name:\n",
    "            return \"jap_100\"\n",
    "        elif \"asia_pacific\" in name:\n",
    "            return \"asia_100\"\n",
    "    elif \"100_portfolios\" in name:\n",
    "        return \"us_100\"\n",
    "    elif \"data_factors\" in name:\n",
    "        return \"us_ff3\"\n",
    "    elif \"5_factors\" in name:\n",
    "        if \"f-f\" in name:\n",
    "            return \"us_ff5\"\n",
    "        elif \"america\" in name:\n",
    "            return \"na_ff5\"\n",
    "        elif \"developed_5_factors\" in name:\n",
    "            return \"dev_ff5\"\n",
    "        elif \"ex_us\" in name:\n",
    "            return \"dev_exUS_ff5\"\n",
    "        elif \"europe\" in name:\n",
    "            return \"euro_ff5\"\n",
    "        elif \"japan\" in name:\n",
    "            return \"jap_ff5\"\n",
    "        elif \"asia_pacific\" in name:\n",
    "            return \"asia_ff5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91cc86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save(df, prefix, start_years, end_year = 2019):\n",
    "    for start_year in start_years:\n",
    "        sub_df = df[(df[\"Year\"] >= start_year) & (df[\"Year\"] <= end_year)]\n",
    "        sub_df.to_csv(f\"clean_data/{frequency}/{prefix}_{start_year}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfa4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_path):\n",
    "    global curr_25_combo\n",
    "    global curr_25_name\n",
    "    \n",
    "    combo_25_pattern = r\"^(.*?25_Portfolios)\"\n",
    "    match_25 = re.search(combo_25_pattern, data_path)\n",
    "    if match_25:\n",
    "        curr_25_name = match_25.group(1)\n",
    "        suffix_pattern = r\"25_Portfolios(.*?)(?:\\d+x\\d+[_]*)?(?:\\.csv|\\.CSV)$\"\n",
    "        suffix = re.search(suffix_pattern, data_path).group(1)\n",
    "        if suffix == \"_\":\n",
    "            suffix = \"5x5\"\n",
    "        if curr_25_combo is None:\n",
    "            curr_25_combo = get_clean_data(data_path)\n",
    "        else:\n",
    "            df = get_clean_data(data_path)\n",
    "            curr_25_combo = pd.merge(curr_25_combo, df, on = [\"Year\", \"Month\"], suffixes = (None, suffix))\n",
    "    elif (\"MOM\" not in data_path) and (\"Momentum\" not in data_path):\n",
    "        if curr_25_combo is not None:\n",
    "            prefix = get_df_name_prefix(curr_25_name)\n",
    "            assert prefix is not None, f\"oopsie, {data_path}\"\n",
    "            split_and_save(curr_25_combo, prefix, [1991, 2008])\n",
    "        curr_25_combo = None\n",
    "        curr_25_name = None\n",
    "        df = get_clean_data(data_path)\n",
    "        prefix = get_df_name_prefix(data_path)\n",
    "        assert prefix is not None, f\"whoopsie, {data_path}\"\n",
    "        split_and_save(df, prefix, [1964, 1991, 2008])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_25_combo = None\n",
    "curr_25_name = None\n",
    "\n",
    "def process_loop():\n",
    "    for file in files:\n",
    "        process_data(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_ff5_and_mom(clean_files):\n",
    "    for file in files:\n",
    "        name = file.lower()\n",
    "        if \"mom\" in name:\n",
    "            if \"f-f\" in name:\n",
    "                tag = \"us_ff5\"\n",
    "            elif \"america\" in name:\n",
    "                tag = \"na_ff5\"\n",
    "            elif \"developed_mom_factor\" in name:\n",
    "                tag = \"dev_ff5\"\n",
    "            elif \"ex_us\" in name:\n",
    "                tag = \"dev_exUS_ff5\"\n",
    "            elif \"europe\" in name:\n",
    "                tag = \"euro_ff5\"\n",
    "            elif \"japan\" in name:\n",
    "                tag = \"jap_ff5\"\n",
    "            elif \"asia_pacific\" in name:\n",
    "                tag = \"asia_ff5\"\n",
    "            momentum_df = get_clean_data(file)\n",
    "            factors_data = [f for f in clean_files if tag in f]\n",
    "            for f in factors_data:\n",
    "                factors_df = pd.read_csv(f\"clean_data/{frequency}/\" + f)\n",
    "                year = f.replace(\".csv\", \"\")[-4:]\n",
    "                combined = pd.merge(factors_df, momentum_df, on = [\"Year\", \"Month\"], suffixes = (None, \"mom\"))\n",
    "                combined.to_csv(f\"clean_data/{frequency}/{tag}_mom_{year}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf493cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5786a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_files = [f for f in listdir(f\"clean_data/{frequency}/\") if isfile(join(f\"clean_data/{frequency}/\", f)) and f.lower().endswith(\".csv\")]\n",
    "clean_files.sort()\n",
    "combine_ff5_and_mom(clean_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ea9b4",
   "metadata": {},
   "source": [
    "Large files:\n",
    "```\n",
    "remote: warning: File clean_data/dev_100_1991.csv is 52.62 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\n",
    "remote: warning: File clean_data/dev_exUS_100_1991.csv is 52.37 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\n",
    "remote: warning: File clean_data/euro_100_1991.csv is 51.17 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\n",
    "remote: warning: File clean_data/jap_100_1991.csv is 50.88 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\n",
    "remote: error: Trace: 5422aa00d681579329a15dff13d01d53b484fcbcb4550009bfc124675159c91d\n",
    "remote: error: See https://gh.io/lfs for more information.\n",
    "remote: error: File clean_data/na_100_2008.csv is 290.15 MB; this exceeds GitHub's file size limit of 100.00 MB\n",
    "remote: error: File clean_data/na_100_1991.csv is 698.29 MB; this exceeds GitHub's file size limit of 100.00 MB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_files = {\n",
    "    \"clean_data/dev_100_1991.csv\": 2,\n",
    "    \"clean_data/dev_exUS_100_1991.csv\": 2,\n",
    "    \"clean_data/euro_100_1991.csv\": 2,\n",
    "    \"clean_data/jap_100_1991.csv\": 2,\n",
    "    \"clean_data/na_100_2008.csv\": 6,\n",
    "    \"clean_data/na_100_1991.csv\": 14\n",
    "}\n",
    "for lf, num_splits in large_files.items():\n",
    "    file_prefix = lf.replace(\"clean_data/\", \"\").replace(\".csv\", \"\")\n",
    "    df = pd.read_csv(lf)\n",
    "    rows_per_split = len(df) // num_splits\n",
    "    for i in range(num_splits):\n",
    "        if i == num_splits - 1:\n",
    "            split_df = df.iloc[i * rows_per_split:]\n",
    "        else:\n",
    "            split_df = df.iloc[i * rows_per_split : (i + 1) * rows_per_split]\n",
    "        split_df.to_csv(f\"clean_data/{file_prefix}_{i + 1}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09b56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
